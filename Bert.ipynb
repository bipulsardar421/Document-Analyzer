{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QigVHZwbYdmz6bciqJPKMb9hIWxyA7w0",
      "authorship_tag": "ABX9TyP8ieHl3VqWcNlnpHehOJaR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bipulsardar421/Document-Analyzer/blob/main/Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyMuPDF scikit-learn matplotlib\n",
        "# !pip install pdf2image\n",
        "\n",
        "# !pip install PyPDF2==2.12.1\n",
        "# !pip install pymupdf\n",
        "# !pip install pytesseract\n",
        "# !sudo apt install tesseract-ocr\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tokenizers\n"
      ],
      "metadata": {
        "id": "7r2wWVQlaI0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "9tdrTyIj_XKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import json\n",
        "import os\n",
        "\n",
        "def extract_text_to_json(pdf_path, output_json_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    json_data = []\n",
        "    for page_num, page in enumerate(doc):\n",
        "        page_data = page.get_text(\"json\")\n",
        "        json_data.append({\n",
        "            \"page\": page_num,\n",
        "            \"data\": json.loads(page_data)\n",
        "        })\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "def process_pdf_folder(folder_path):\n",
        "\n",
        "    pdf_files = []\n",
        "\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "\n",
        "        if os.path.isfile(file_path) and file_path.lower().endswith('.pdf'):\n",
        "            pdf_files.append(file_path)\n",
        "\n",
        "    for pdf_path in pdf_files:\n",
        "\n",
        "        json_output_path = os.path.splitext(pdf_path)[0] + '.json'\n",
        "\n",
        "\n",
        "        extract_text_to_json(pdf_path, json_output_path)\n",
        "        print(f\"Extracted text from '{pdf_path}' to '{json_output_path}'\")\n",
        "\n",
        "\n",
        "quote_folder = \"/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins\"\n",
        "process_pdf_folder(quote_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJyyGJV_bQe4",
        "outputId": "19089aee-a3e1-4da5-afa7-e81de02034a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 QUOTE Excess $15M xs$10M Security Nat_l frm Amwins.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 QUOTE Excess $15M xs$10M Security Nat_l frm Amwins.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB EvanstonAmWins (Bound) (2).pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB EvanstonAmWins (Bound) (2).json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 03 QUO CUMB EvanstonAmwins $708.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 03 QUO CUMB EvanstonAmwins $708.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 12 QUO GLIA AMWINS.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 12 QUO GLIA AMWINS.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 BIND CUMB.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 BIND CUMB.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 QUO CUMB.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 QUO CUMB.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2024 02 QUO UMB Evanston $661.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2024 02 QUO UMB Evanston $661.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 06 QUO CUMB.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 06 QUO CUMB.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB EvanstonAmWins (Bound).pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB EvanstonAmWins (Bound).json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB-1.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB-1.json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB EvanstonAmWins (Bound).pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB EvanstonAmWins (Bound).json'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO UMB.pdf' to '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO UMB.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def extract_text_to_json(pdf_path, output_json_dir):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    json_data = []\n",
        "    for page_num, page in enumerate(doc):\n",
        "        page_data = page.get_text(\"json\")\n",
        "        json_data.append({\n",
        "            \"page\": page_num,\n",
        "            \"data\": json.loads(page_data)\n",
        "        })\n",
        "\n",
        "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "\n",
        "    output_json_path = os.path.join(output_json_dir, f\"{filename}.json\")\n",
        "\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "def process_pdf_folder(folder_path, output_json_dir):\n",
        "\n",
        "    pdf_files = []\n",
        "\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        if os.path.isfile(file_path) and file_path.lower().endswith('.pdf'):\n",
        "            pdf_files.append(file_path)\n",
        "\n",
        "\n",
        "    for pdf_path in pdf_files:\n",
        "\n",
        "        extract_text_to_json(pdf_path, output_json_dir)\n",
        "        print(f\"Extracted text from '{pdf_path}' to '{output_json_dir}'\")\n",
        "\n",
        "\n",
        "quote_folder = \"/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins\"\n",
        "output_json_dir = \"/content/sample_data/jsonFile\"\n",
        "process_pdf_folder(quote_folder, output_json_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-QOPKtqFqA4",
        "outputId": "c64130bd-17ce-405e-e9bf-3fcab7183860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 QUOTE Excess $15M xs$10M Security Nat_l frm Amwins.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB EvanstonAmWins (Bound) (2).pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 03 QUO CUMB EvanstonAmwins $708.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 12 QUO GLIA AMWINS.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 BIND CUMB.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 07 QUO CUMB.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2024 02 QUO UMB Evanston $661.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 06 QUO CUMB.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO CUMB EvanstonAmWins (Bound).pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB-1.pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 05 QUO CUMB EvanstonAmWins (Bound).pdf' to '/content/sample_data/jsonFile'\n",
            "Extracted text from '/content/drive/MyDrive/Training Files/Task_Bipul/Quote/Amwins/2023 02 QUO UMB.pdf' to '/content/sample_data/jsonFile'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def extract_text_to_json(pdf_path, output_json_dir):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    json_data = []\n",
        "    for page_num, page in enumerate(doc):\n",
        "        page_data = page.get_text(\"json\")\n",
        "        json_data.append({\n",
        "            \"page\": page_num,\n",
        "            \"data\": json.loads(page_data)\n",
        "        })\n",
        "\n",
        "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "\n",
        "    output_json_path = os.path.join(output_json_dir, f\"{filename}.json\")\n",
        "\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "def process_quote_folder(quote_folder):\n",
        "\n",
        "    for root, _, files in os.walk(quote_folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                pdf_path = os.path.join(root, file)\n",
        "\n",
        "                extract_text_to_json(pdf_path, root)\n",
        "                print(f\"Extracted text from '{pdf_path}' to '{root}'\")\n",
        "\n",
        "\n",
        "quote_folder = \"/content/drive/MyDrive/Training Files/Task_Bipul/Quote\"\n",
        "process_quote_folder(quote_folder)\n"
      ],
      "metadata": {
        "id": "y67qmHKnHBin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import os\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Set the path to the Tesseract executable (change this path as needed)\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "# Function to perform OCR on an image\n",
        "def perform_ocr(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "# Function to extract text and layout from a single PDF file and save it as JSON\n",
        "def extract_text_to_json(pdf_path, output_json_dir):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    json_data = []\n",
        "\n",
        "    for page_num, page in enumerate(doc):\n",
        "        images_list = page.get_images(full=True)\n",
        "\n",
        "        if images_list:\n",
        "            for img_idx, img_info in enumerate(images_list):\n",
        "                xref = img_info[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_extension = base_image[\"ext\"]\n",
        "                image_filename = f\"{pdf_path}_page{page_num+1}_image{img_idx+1}.{image_extension}\"\n",
        "                image_path = os.path.join(output_images_dir, image_filename)\n",
        "\n",
        "                with open(image_path, \"wb\") as image_file:\n",
        "                    image_file.write(image_bytes)\n",
        "\n",
        "                ocr_text = perform_ocr(image_path)\n",
        "                json_data.append({\n",
        "                    \"page\": page_num,\n",
        "                    \"image_index\": img_idx,\n",
        "                    \"ocr_text\": ocr_text\n",
        "                })\n",
        "\n",
        "        page_data = page.get_text(\"json\")\n",
        "        json_data.append({\n",
        "            \"page\": page_num,\n",
        "            \"data\": json.loads(page_data)\n",
        "        })\n",
        "\n",
        "    # Extract filename without extension\n",
        "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "    # Construct output JSON file path in the same directory as PDF\n",
        "    output_json_path = os.path.join(output_json_dir, f\"{filename}.json\")\n",
        "\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Function to process all PDF files in a directory and its subdirectories\n",
        "def process_quote_folder(quote_folder):\n",
        "    # Iterate through the main directory and its subdirectories\n",
        "    for root, _, files in os.walk(quote_folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                pdf_path = os.path.join(root, file)\n",
        "                # Save JSON file in the same directory as PDF\n",
        "                extract_text_to_json(pdf_path, root)\n",
        "                print(f\"Extracted text from '{pdf_path}' to '{root}'\")\n",
        "\n",
        "# Example usage:\n",
        "quote_folder = \"/content/drive/MyDrive/Training Files/Task_Bipul\"\n",
        "output_images_dir = \"/Task_Bipul/OCR_Images\"\n",
        "process_quote_folder(quote_folder)\n"
      ],
      "metadata": {
        "id": "P2_K0KFEISEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Function to create the new JSON structure\n",
        "def create_json_metadata(root_folder, output_json_path):\n",
        "    document_types = [\"Quote\", \"Binder\", \"Accord\", \"Endorsement\"]\n",
        "    metadata = []\n",
        "\n",
        "    for document_type in document_types:\n",
        "        document_type_folder = os.path.join(root_folder, document_type)\n",
        "        if not os.path.exists(document_type_folder):\n",
        "            continue\n",
        "\n",
        "        for company in os.listdir(document_type_folder):\n",
        "            company_folder = os.path.join(document_type_folder, company)\n",
        "            if not os.path.isdir(company_folder):\n",
        "                continue\n",
        "\n",
        "            for file in os.listdir(company_folder):\n",
        "                if file.lower().endswith('.json'):\n",
        "                    file_path = os.path.join(company_folder, file)\n",
        "                    metadata.append({\n",
        "                        \"file_name\": file,\n",
        "                        \"document_type\": document_type,\n",
        "                        \"company\": company\n",
        "                    })\n",
        "\n",
        "    # Write metadata to the output JSON file\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as output_file:\n",
        "        json.dump(metadata, output_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Metadata JSON created at: {output_json_path}\")\n",
        "\n",
        "# Example usage\n",
        "root_folder = \"/content/drive/MyDrive/Training Files/Task_Bipul\"\n",
        "output_json_path = \"/content/drive/MyDrive/Training Files/Task_Bipul/document_metadata.json\"\n",
        "create_json_metadata(root_folder, output_json_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm0EPTyGt3K_",
        "outputId": "f688988b-fc43-4e41-90d8-7431964839fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata JSON created at: /content/drive/MyDrive/Training Files/Task_Bipul/document_metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Function to create the new JSON structure\n",
        "def create_json_metadata(root_folder, output_json_path):\n",
        "    document_types = [\"Quote\", \"Binder\", \"Accord\", \"Endorsement\"]\n",
        "    metadata = []\n",
        "\n",
        "    for document_type in document_types:\n",
        "        document_type_folder = os.path.join(root_folder, document_type)\n",
        "        if not os.path.exists(document_type_folder):\n",
        "            continue\n",
        "\n",
        "        for company in os.listdir(document_type_folder):\n",
        "            company_folder = os.path.join(document_type_folder, company)\n",
        "            if not os.path.isdir(company_folder):\n",
        "                continue\n",
        "\n",
        "            for file in os.listdir(company_folder):\n",
        "                if file.lower().endswith('.json'):\n",
        "                    file_path = os.path.join(company_folder, file)\n",
        "                    relative_file_path = os.path.relpath(file_path, root_folder)\n",
        "                    metadata.append({\n",
        "                        \"file_name\": relative_file_path,\n",
        "                        \"document_type\": document_type,\n",
        "                        \"company\": company\n",
        "                    })\n",
        "\n",
        "    # Write metadata to the output JSON file\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as output_file:\n",
        "        json.dump(metadata, output_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Metadata JSON created at: {output_json_path}\")\n",
        "\n",
        "# Example usage\n",
        "root_folder = \"/content/drive/MyDrive/Training Files/Task_Bipul\"\n",
        "output_json_path = \"/content/drive/MyDrive/Training Files/Task_Bipul/document_metadata.json\"\n",
        "create_json_metadata(root_folder, output_json_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6A7atYf_bbe",
        "outputId": "6d230543-4ff1-4b65-9bad-d1e8fb334c1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata JSON created at: /content/drive/MyDrive/Training Files/Task_Bipul/document_metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "# Load metadata\n",
        "with open('/content/drive/MyDrive/Training Files/Task_Bipul/document_metadata.json') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# Extract and preprocess data\n",
        "def extract_features(document):\n",
        "    # Custom function to extract relevant features from the document\n",
        "    pass\n",
        "\n",
        "# Prepare training data\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for item in metadata:\n",
        "    document_path = item['path']\n",
        "    with open(document_path) as doc:\n",
        "        text = doc.read()\n",
        "    features = extract_features(text)\n",
        "    train_texts.append(features)\n",
        "    train_labels.append((item['company'], item['type']))\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_labels)))\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "\n",
        "# Define dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/content/drive/MyDrive/Logs',\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Classification and Segregation Script\n",
        "def classify_and_segregate(document_path):\n",
        "    with open(document_path) as doc:\n",
        "        text = doc.read()\n",
        "    features = extract_features(text)\n",
        "    encodings = tokenizer(features, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    outputs = model(**encodings)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "    company, doc_type = train_labels[predictions]\n",
        "    return {\"company name\": company, \"type\": doc_type, \"documentName\": os.path.basename(document_path)}\n",
        "\n",
        "# Automate segregation\n",
        "new_documents = ['new_doc1.json', 'new_doc2.json']  # List of new documents\n",
        "output = []\n",
        "for doc_path in new_documents:\n",
        "    result = classify_and_segregate(doc_path)\n",
        "    output.append(result)\n",
        "    company_folder = os.path.join('segregated_documents', result['company name'])\n",
        "    type_folder = os.path.join(company_folder, result['type'])\n",
        "    os.makedirs(type_folder, exist_ok=True)\n",
        "    shutil.move(doc_path, os.path.join(type_folder, os.path.basename(doc_path)))\n",
        "\n",
        "# Save output JSON\n",
        "with open('segregation_output.json', 'w') as f:\n",
        "    json.dump(output, f, indent=4)\n"
      ],
      "metadata": {
        "id": "6TzGNbyBLtCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}